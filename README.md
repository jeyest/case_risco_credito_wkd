# case_risco_credito_wkd
Case completo de Engenharia de Dados e Modelagem de Risco de CrÃ©dito usando SQL (PostgreSQL), Python, EDA e Machine Learning. Inclui pipeline incremental, versÃ£o one-shot, tabela final e preparaÃ§Ã£o para modelagem.

Este projeto faz parte da formaÃ§Ã£o:

**FormaÃ§Ã£o Cientista de Dados: O Curso Completo - 2025 (Fernando Amaral)**  
Plataforma: Udemy

O case foi expandido com boas prÃ¡ticas de Engenharia de Dados, Git, documentaÃ§Ã£o estruturada e preparaÃ§Ã£o avanÃ§ada para modelagem de risco de crÃ©dito.

--

## Objetivo do projeto

Este case implementa um pipeline completo de engenharia de dados e modelagem de risco de crÃ©dito.
Abrange desde a ingestÃ£o e manipulaÃ§Ã£o de tabelas normalizadas atÃ© a construÃ§Ã£o de uma tabela analÃ­tica final, preparada para anÃ¡lises estatÃ­sticas e aplicaÃ§Ã£o de modelos de machine learning.

O objetivo Ã© demonstrar, de forma integrada, todas as etapas necessÃ¡rias para transformar dados brutos em informaÃ§Ãµes prontas para uso em processos de decisÃ£o e modelagem preditiva.

O projeto aplica:

- boas prÃ¡ticas de Data Engineering  
- versionamento Git  
- SQL estruturado (incremental e one-shot)  
- criaÃ§Ã£o de dataset final `TB_CREDITO`  
- preparaÃ§Ã£o para EDA e modelagem em Python

---

## ğŸ—‚ Estrutura do RepositÃ³rio

case_risco_credito_wkd/
â”‚
â”œâ”€â”€ sql/
â”‚ â”œâ”€â”€ pipeline_sql_incremental.sql # Pipeline incremental usando views
â”‚ â”œâ”€â”€ pipeline_sql_one_shot.sql # Pipeline completo em join Ãºnico
â”‚ â””â”€â”€ .gitkeep
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ .gitkeep # Arquivos CSV ou dumps (a preencher)
â”‚
â”œâ”€â”€ python/
â”‚ â””â”€â”€ .gitkeep # Scripts e notebooks de modelagem (em breve)
â”‚
â”œâ”€â”€ docs/
â”‚ â””â”€â”€ .gitkeep # DocumentaÃ§Ã£o complementar
â”‚
â””â”€â”€ README.md

---

## Tecnologias Utilizadas

- **PostgreSQL** â€“ base relacional e enriquecimento de dados  
- **SQL** â€“ joins, materializaÃ§Ãµes, views e limpeza  
- **Git & GitHub** â€“ versionamento e organizaÃ§Ã£o do repositÃ³rio  
- **Python (futuro)** â€“ EDA, feature engineering, modelagem e mÃ©tricas  

---

## Pipeline Incremental (Views)

O arquivo: sql/pipeline_sql_incremental.sql

ContÃ©m 9 etapas sequenciais, cada uma adicionando uma dimensÃ£o ao dataset principal.

As views formam o fluxo:

CREDITO
â†’ vw_credito_1 (histÃ³rico)
â†’ vw_credito_2 (propÃ³sito)
â†’ vw_credito_3 (investimentos)
â†’ vw_credito_4 (emprego)
â†’ vw_credito_5 (estado civil)
â†’ vw_credito_6 (fiador)
â†’ vw_credito_7 (habitaÃ§Ã£o)
â†’ vw_credito_8 (outros financiamentos)
â†’ vw_credito_9 (profissÃ£o)


### Por que usar incremental?

- facilita debugging  
- permite validaÃ§Ã£o etapa a etapa  
- deixa o pipeline mais didÃ¡tico e auditÃ¡vel  

---

## Pipeline One-Shot

O arquivo: sql/pipeline_sql_one_shot.sql

Implementa uma abordagem otimizada em duas etapas:

### 1. TB_CREDITO_BRUTO  
Join Ãºnico com todas as tabelas dimensÃ£o.

### 2. TB_CREDITO  
Tabela final, com nomes padronizados, pronta para anÃ¡lise e modelagem.

### âœ” Por que usar one-shot?

- Ãºtil para cargas completas (full load)  
- ideal para Data Warehouse e Lakehouse  
- simplifica a materializaÃ§Ã£o final  

---

## Tabela Final: TB_CREDITO

A tabela contÃ©m variÃ¡veis categÃ³ricas e numÃ©ricas sobre:

- perfil do cliente  
- situaÃ§Ã£o socioeconÃ´mica  
- caracterÃ­sticas do crÃ©dito  
- fatores de risco  
- variÃ¡vel-alvo (`target`) indicando inadimplÃªncia  

Essa tabela serÃ¡ utilizada no Python para:

- EDA  
- construÃ§Ã£o de variÃ¡veis  
- modelagem preditiva  
- mÃ©tricas de risco (ROC, Gini, KS, AUC)  

---

## PrÃ³ximos Passos (Python)

O diretÃ³rio `/python` receberÃ¡:

### 1. Carregamento da TB_CREDITO via pandas  
### 2. EDA completa (boxplot, distplots, correlaÃ§Ãµes)  
### 3. Tratamento de valores ausentes  
### 4. Feature engineering  
### 5. Modelagem:  
- RegressÃ£o LogÃ­stica  
- Ãrvore  
- Random Forest  
- Gradient Boosting  
- XGBoost / LightGBM  

### 6. AvaliaÃ§Ã£o de modelos  
### 7. Explainability (SHAP)

---

## Contato

**Autor:** JEYEST (Jeislan Carlos de Souza)  
RepositÃ³rio criado para fins educacionais e demonstraÃ§Ã£o de boas prÃ¡ticas de Engenharia e CiÃªncia de Dados.





